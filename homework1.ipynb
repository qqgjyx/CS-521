{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q1",
   "id": "edb73233e27d3dc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T05:50:06.549487Z",
     "start_time": "2024-09-16T05:50:05.815195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matlab.engine\n",
    "\n",
    "# Helper function to load and analyze a graph\n",
    "def analyze_graph(file_path, network_name):\n",
    "    # Step 1: Load the undirected graph using networkx\n",
    "    G_nx = nx.read_edgelist(file_path, create_using=nx.Graph(), nodetype=int)\n",
    "\n",
    "    # Step 2: Check the basic properties of the graph\n",
    "    num_nodes = G_nx.number_of_nodes()\n",
    "    num_edges = G_nx.number_of_edges()\n",
    "    \n",
    "    # Degree statistics\n",
    "    degrees = np.array([deg for (node, deg) in G_nx.degree()])\n",
    "    max_degree = np.max(degrees)\n",
    "    min_degree = np.min(degrees)\n",
    "    avg_degree = np.mean(degrees)\n",
    "\n",
    "    # Degree distribution type (simplified: scale-free or unknown)\n",
    "    if np.max(degrees) > 10 * np.mean(degrees):  # rough heuristic for scale-free\n",
    "        degree_dist_type = 'Scale-free'\n",
    "    else:\n",
    "        degree_dist_type = 'Unknown'\n",
    "\n",
    "    # Step 3: Check for connectivity (undirected graph)\n",
    "    is_connected = nx.is_connected(G_nx)\n",
    "    if not is_connected:\n",
    "        diameter = 'N/A - Graph not connected'\n",
    "    else:\n",
    "        diameter = 'Good'  # Diameter only makes sense for connected graphs\n",
    "        \n",
    "\n",
    "    # Print out the required information\n",
    "    print(f\"--- {network_name} ---\")\n",
    "    print(f\"n (Number of nodes): {num_nodes}\")\n",
    "    print(f\"m (Number of edges): {num_edges}\")\n",
    "    print(f\"min(d) (Minimum degree): {min_degree}\")\n",
    "    print(f\"max(d) (Maximum degree): {max_degree}\")\n",
    "    print(f\"avg(d) (Average degree): {avg_degree:.2f}\")\n",
    "    print(f\"Degree distribution type: {degree_dist_type}\")\n",
    "    print(f\"Diameter: {diameter}\")\n",
    "    print()\n",
    "\n",
    "    # Step 4: Compute the Local Clustering Coefficient (LCC) for each node\n",
    "    lcc_dict = nx.clustering(G_nx)\n",
    "    lcc_values = np.array(list(lcc_dict.values()))  # Convert to numpy array\n",
    "\n",
    "    return degrees, lcc_values\n",
    "\n",
    "# Helper function to plot distributions using MATLAB\n",
    "def plot_with_matlab(eng, degree, lcc_values, network_name):\n",
    "    nbins = 50\n",
    "    \n",
    "    # Plot Degree Distribution\n",
    "    eng.workspace['d'] = degree\n",
    "    figID = 1\n",
    "    eng.eval(f\"plot_distribution(d, '{network_name} Degree Distribution', {nbins}, {figID})\", nargout=0)\n",
    "    \n",
    "    # Plot LCC Distribution\n",
    "    eng.workspace['lcc'] = lcc_values\n",
    "    figID = 2\n",
    "    eng.eval(f\"plot_distribution(lcc, '{network_name} LCC Distribution', {nbins}, {figID})\", nargout=0)\n"
   ],
   "id": "2c4dfbec05e243be",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T03:18:35.681854Z",
     "start_time": "2024-09-16T03:18:24.392905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.eval(\"addpath(genpath('Mcodes'))\", nargout=0)\n",
    "\n",
    "file_path_amazon = 'data/com-amazon.ungraph/com-amazon.ungraph.txt'\n",
    "degree_amazon, lcc_amazon = analyze_graph(file_path_amazon, \"Amazon\")\n",
    "plot_with_matlab(eng, degree_amazon, lcc_amazon, \"Amazon\")"
   ],
   "id": "665773646a9f9350",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Amazon ---\n",
      "n (Number of nodes): 334863\n",
      "m (Number of edges): 925872\n",
      "min(d) (Minimum degree): 1\n",
      "max(d) (Maximum degree): 549\n",
      "avg(d) (Average degree): 5.53\n",
      "Degree distribution type: Scale-free\n",
      "Diameter: Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T02:07:12.640563Z",
     "start_time": "2024-09-16T02:07:10.978357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "file_path_facebook = 'data/facebook/facebook_combined.txt'\n",
    "degree_facebook, lcc_facebook = analyze_graph(file_path_facebook, \"Facebook\")\n",
    "plot_with_matlab(eng, degree_facebook, lcc_facebook, \"Facebook\")"
   ],
   "id": "9248dfa539547f01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Facebook ---\n",
      "n (Number of nodes): 4039\n",
      "m (Number of edges): 88234\n",
      "min(d) (Minimum degree): 1\n",
      "max(d) (Maximum degree): 1045\n",
      "avg(d) (Average degree): 43.69\n",
      "Degree distribution type: Scale-free\n",
      "Diameter: Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T02:08:02.516029Z",
     "start_time": "2024-09-16T02:07:55.465936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "file_path_dblp = 'data/COM-DBLP/com-dblp.ungraph.txt'\n",
    "degree_dblp, lcc_dblp = analyze_graph(file_path_dblp, \"DBLP\")\n",
    "plot_with_matlab(eng, degree_dblp, lcc_dblp, \"DBLP\")\n",
    "\n"
   ],
   "id": "a346d307ae854925",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DBLP ---\n",
      "n (Number of nodes): 317080\n",
      "m (Number of edges): 1049866\n",
      "min(d) (Minimum degree): 1\n",
      "max(d) (Maximum degree): 343\n",
      "avg(d) (Average degree): 6.62\n",
      "Degree distribution type: Scale-free\n",
      "Diameter: Good\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T02:08:37.608433Z",
     "start_time": "2024-09-16T02:08:37.595437Z"
    }
   },
   "cell_type": "code",
   "source": "eng.exit()",
   "id": "afc96474ac5c012c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q2",
   "id": "df909a38b582fb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T04:23:43.302992Z",
     "start_time": "2024-09-16T04:23:41.100574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Helper function to load G_1\n",
    "\n",
    "\n",
    "def load_g1(file_path):\n",
    "    # Load the undirected graph using networkx\n",
    "    G_nx = nx.read_edgelist(file_path, create_using=nx.Graph(), nodetype=int)\n",
    "    return G_nx\n",
    "\n",
    "\n",
    "# New function to compute the G_2 graph from G_1\n",
    "def construct_g2_sparse(G_nx):\n",
    "    # Compute the sparse adjacency matrix of G_1\n",
    "    A1_sparse = nx.to_scipy_sparse_array(G_nx)  # This is the correct method\n",
    "    \n",
    "    # Compute A_2 = A_1^2 (2-step connectivity) using sparse matrix multiplication\n",
    "    A2_sparse = A1_sparse.dot(A1_sparse)\n",
    "\n",
    "    # Create the G_2 graph from the sparse matrix\n",
    "    G2_nx = nx.from_scipy_sparse_array(A2_sparse)\n",
    "\n",
    "    return G2_nx\n",
    "\n",
    "\n",
    "# Helper function to analyze a graph (G_2)\n",
    "def analyze_g2(G2_nx, network_name):\n",
    "    # Degree statistics\n",
    "    degrees = np.array([deg for (node, deg) in G2_nx.degree()])\n",
    "    max_degree = np.max(degrees)\n",
    "    min_degree = np.min(degrees)\n",
    "    avg_degree = np.mean(degrees)\n",
    "\n",
    "    # # Degree distribution type (simplified: scale-free or unknown)\n",
    "    # if np.max(degrees) > 10 * np.mean(degrees):  # rough heuristic for scale-free\n",
    "    #     degree_dist_type = 'Scale-free'\n",
    "    # else:\n",
    "    #     degree_dist_type = 'Unknown'\n",
    "\n",
    "    # Step 3: Check for connectivity (undirected graph)\n",
    "    # is_connected = nx.is_connected(G2_nx)\n",
    "    # diameter = nx.diameter(G2_nx) if is_connected else 'N/A - Graph not connected'\n",
    "\n",
    "    # Print out the required information\n",
    "    print(f\"--- {network_name} (G2) ---\")\n",
    "    print(f\"min(d) (Minimum degree): {min_degree}\")\n",
    "    print(f\"max(d) (Maximum degree): {max_degree}\")\n",
    "    print(f\"avg(d) (Average degree): {avg_degree:.2f}\")\n",
    "    # print(f\"Degree distribution type: {degree_dist_type}\")\n",
    "    # print(f\"Diameter: {diameter}\")\n",
    "    print()\n",
    "\n",
    "    # Step 4: Compute the Local Clustering Coefficient (LCC) for each node\n",
    "    lcc_dict = nx.clustering(G2_nx)\n",
    "    lcc_values = np.array(list(lcc_dict.values()))  # Convert to numpy array\n",
    "\n",
    "    return degrees, lcc_values\n",
    "\n",
    "\n",
    "# Helper function to plot distributions using MATLAB\n",
    "def plot_with_matlab(eng, degree, lcc_values, network_name, graph_label=\"G2\"):\n",
    "    nbins = 50\n",
    "\n",
    "    # Plot Degree Distribution\n",
    "    eng.workspace['d'] = degree\n",
    "    figID = 1\n",
    "    eng.eval(f\"plot_distribution(d, '{network_name} Degree Distribution ({graph_label})', {nbins}, {figID})\", nargout=0)\n",
    "\n",
    "    # Plot LCC Distribution\n",
    "    eng.workspace['lcc'] = lcc_values\n",
    "    figID = 2\n",
    "    eng.eval(f\"plot_distribution(lcc, '{network_name} LCC Distribution ({graph_label})', {nbins}, {figID})\", nargout=0)\n",
    "\n",
    "\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.eval(\"addpath(genpath('Mcodes'))\", nargout=0)"
   ],
   "id": "6887a1ac93bcd176",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T04:10:40.630856Z",
     "start_time": "2024-09-16T04:09:01.906885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "file_path_amazon = 'data/com-amazon.ungraph/com-amazon.ungraph.txt'\n",
    "G1_amazon = load_g1(file_path_amazon)  # Load G1 (recompute)\n",
    "\n",
    "G2_amazon = construct_g2_sparse(G1_amazon)  # Construct G2 from G1\n",
    "degree_amazon_g2, lcc_amazon_g2 = analyze_g2(G2_amazon, \"Amazon\")  # Analyze G2\n",
    "\n",
    "# Plot results for G2 (Amazon)\n",
    "plot_with_matlab(eng, degree_amazon_g2, lcc_amazon_g2, \"Amazon\", \"G2\")"
   ],
   "id": "9ae3c162b6511c1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Amazon (G2) ---\n",
      "min(d) (Minimum degree): 3\n",
      "max(d) (Maximum degree): 1328\n",
      "avg(d) (Average degree): 42.15\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T04:08:18.264339Z",
     "start_time": "2024-09-16T04:06:58.893223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "file_path_facebook = 'data/facebook/facebook_combined.txt'\n",
    "G1_facebook = load_g1(file_path_facebook)  # Load G1 (recompute)\n",
    "\n",
    "G2_facebook = construct_g2_sparse(G1_facebook)  # Construct G2 from G1\n",
    "degree_facebook_g2, lcc_facebook_g2 = analyze_g2(G2_facebook, \"Facebook\")  # Analyze G2\n",
    "\n",
    "# Plot results for G2 (Facebook)\n",
    "plot_with_matlab(eng, degree_facebook_g2, lcc_facebook_g2, \"Facebook\", \"G2\")"
   ],
   "id": "cd58ee7c9b1e0084",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Facebook (G2) ---\n",
      "min(d) (Minimum degree): 58\n",
      "max(d) (Maximum degree): 2916\n",
      "avg(d) (Average degree): 718.13\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T04:32:41.030802Z",
     "start_time": "2024-09-16T04:23:45.955293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "file_path_dblp = 'data/COM-DBLP/com-dblp.ungraph.txt'\n",
    "G1_dblp = load_g1(file_path_dblp)  # Load G1 (recompute)\n",
    "\n",
    "G2_dblp = construct_g2_sparse(G1_dblp)  # Construct G2 from G1\n",
    "degree_dblp_g2, lcc_dblp_g2 = analyze_g2(G2_dblp, \"DBLP\")  # Analyze G2\n",
    "\n",
    "# Plot results for G2 (DBLP)\n",
    "plot_with_matlab(eng, degree_dblp_g2, lcc_dblp_g2, \"DBLP\", \"G2\")"
   ],
   "id": "dce9d449fa47c755",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DBLP (G2) ---\n",
      "min(d) (Minimum degree): 3\n",
      "max(d) (Maximum degree): 5396\n",
      "avg(d) (Average degree): 88.07\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "eng.exit()"
   ],
   "id": "d8c8964fc6d4aa1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q3",
   "id": "5d419e3b28597b5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T05:50:13.122627Z",
     "start_time": "2024-09-16T05:50:10.535337Z"
    }
   },
   "cell_type": "code",
   "source": "# See .mlx",
   "id": "a63a49b375810e17",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q4",
   "id": "5ef59e9a7077bf19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:04:49.725496Z",
     "start_time": "2024-09-16T10:04:43.691481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.pyC4H import *\n",
    "from src.pyP2G import *\n",
    "from src.whuHIloader import *\n",
    "import matlab.engine\n",
    "\n",
    "# Helper function to plot distributions using MATLAB\n",
    "def plot_with_matlab(eng, degree, lcc_values=None, network_name=None, graph_label=\"G2\"):\n",
    "    nbins = 50\n",
    "\n",
    "    # Plot Degree Distribution\n",
    "    eng.workspace['d'] = degree\n",
    "    figID = 1\n",
    "    eng.eval(f\"plot_distribution(d, '{network_name} In Degree Distribution ({graph_label})', {nbins}, {figID})\", nargout=0)\n",
    "    \n",
    "    # Plot LCC Distribution\n",
    "    eng.workspace['lcc'] = lcc_values\n",
    "    figID = 2\n",
    "    eng.eval(f\"plot_distribution(lcc, '{network_name} aLCC Distribution ({graph_label})', {nbins}, {figID})\", nargout=0)\n",
    "    \n",
    "def fast_local_clustering_coefficient(A):\n",
    "    \"\"\"\n",
    "    Fast calculation of the local clustering coefficient using A^3.\n",
    "    This method works for both weighted and unweighted adjacency matrices.\n",
    "    \"\"\"\n",
    "    # Ensure A is in CSR format for efficient row operations\n",
    "    if not isinstance(A, csr_matrix):\n",
    "        A = csr_matrix(A)\n",
    "\n",
    "    # Step 1: Compute A^3 (A dot A dot A)\n",
    "    A_cubed = A.dot(A).dot(A)\n",
    "\n",
    "    # Step 2: Extract diagonal of A^3 (number of triangles passing through each node)\n",
    "    triangles = A_cubed.diagonal()\n",
    "\n",
    "    # Step 3: Calculate node degrees (number of neighbors)\n",
    "    degrees = np.array(A.sum(axis=1)).flatten()\n",
    "\n",
    "    # Step 4: Calculate local clustering coefficient for each node\n",
    "    local_clustering = []\n",
    "    for i in range(A.shape[0]):\n",
    "        k_i = degrees[i]  # Degree of node i\n",
    "        if k_i < 2:\n",
    "            local_clustering.append(0)\n",
    "            continue\n",
    "        # Possible triangles is k_i * (k_i - 1) / 2\n",
    "        possible_triangles = (k_i * (k_i - 1)) / 2\n",
    "        # Actual triangles come from diag(A^3)\n",
    "        local_clustering.append(triangles[i] / possible_triangles if possible_triangles > 0 else 0)\n",
    "\n",
    "    return np.array(local_clustering)\n",
    "\n",
    "method_name = 'kNN'\n",
    "feature_para_pair = [None]\n",
    "param_grid = {\n",
    "    'spatial_embedding': [True],\n",
    "    'pca_components': [12],\n",
    "    'ksize': [None],\n",
    "    'n_neighbors': [100]  # k = 100 for kNN\n",
    "}\n",
    "k = 44\n",
    "\n",
    "eng = matlab.engine.start_matlab()\n",
    "eng.eval(\"addpath(genpath('Mcodes'))\", nargout=0)"
   ],
   "id": "dc459750fdbdecde",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:06:19.861669Z",
     "start_time": "2024-09-16T10:04:50.710053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "dataset_name = 'WHU_Hi_HongHu'\n",
    "hsi_data, gt = whuHi_load(f\"data/Matlab_data_format/Matlab_data_format/WHU-Hi-HongHu/Training samples and test samples\", f\"data/Matlab_data_format/Matlab_data_format/WHU-Hi-HongHu/{dataset_name}_gt\", var_header='HHCY')\n",
    "ground_truth = gt.reshape(-1)\n",
    "process_dataset(dataset_name, hsi_data, ground_truth, method_name, param_grid, feature_para_pair, no_tuning=False, threshold=0.017)\n",
    "\n",
    "# Load the data from CSV files\n",
    "distances_file = 'WHU_Hi_HongHu_knn_spatial_embedding_pca_12_no_kernel_distances.csv'\n",
    "indices_file = 'WHU_Hi_HongHu_knn_spatial_embedding_pca_12_no_kernel_indices.csv'\n",
    "dataset_name = 'WHU_Hi_HongHu'\n",
    "hsi_data, gt = whuHi_load(f\"data/Matlab_data_format/Matlab_data_format/WHU-Hi-HongHu/Training samples and test samples\", f\"data/Matlab_data_format/Matlab_data_format/WHU-Hi-HongHu/{dataset_name}_gt\", var_header='HHCY')\n",
    "ground_truth = gt.reshape(-1)\n",
    "# Read the CSV files\n",
    "distances = pd.read_csv(distances_file, header=None, low_memory=False)\n",
    "indices = pd.read_csv(indices_file, header=None, low_memory=False)\n",
    "indices = np.array(indices)\n",
    "distances = np.array(distances)\n",
    "\n",
    "# Use the existing functions to create the k-nearest neighbors graph\n",
    "A = create_knn_graph(indices, distances, k)\n",
    "\n",
    "# Apply the assertion function to the generated matrix\n",
    "try:\n",
    "    assert_graph_requirements(A, k)\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    print(\"Matrix A:\\n\", A)\n",
    "    print(\"Non-zero elements per column:\", (A > 0).sum(axis=0))\n",
    "    \n",
    "# Calculate in-degree by counting non-zero entries in each column\n",
    "in_degrees = np.sum(A > 0, axis=1)\n",
    "local_density = fast_local_clustering_coefficient(A)\n",
    "plot_with_matlab(eng, in_degrees, local_density, network_name=f\"WHU-Hi-HongHu\")"
   ],
   "id": "cf7047cfb7babcff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing kNN for WHU_Hi_HongHu:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fef509be88994e49abfdef967347cb0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the adjacency matrix: 0.9999014557670772\n",
      "1. All weights are positive: PASS\n",
      "2. Each column has exactly 44 non-zero elements: PASS\n",
      "3. Matrix is column-stochastic: PASS\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:08:14.465093Z",
     "start_time": "2024-09-16T10:08:14.463063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "#\n",
    "#"
   ],
   "id": "282448e33dc6545e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:08:18.249001Z",
     "start_time": "2024-09-16T10:08:15.618065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = 'SalinasA'\n",
    "data_path = 'data/SalinasA/SalinasA_corrected.mat'\n",
    "var_name = 'salinasA_corrected'\n",
    "gt_path = 'data/SalinasA/SalinasA_gt.mat'\n",
    "gt_var_name = 'salinasA_gt'\n",
    "\n",
    "hsi_data = load_hsi_data(data_path, var_name)\n",
    "ground_truth = load_hsi_data(gt_path, gt_var_name).reshape(-1)\n",
    "\n",
    "process_dataset(dataset_name, hsi_data, ground_truth, method_name, param_grid, feature_para_pair, no_tuning=False, threshold=0.03)\n",
    "# Load the data from CSV files\n",
    "distances_file = 'SalinasA_knn_spatial_embedding_pca_12_no_kernel_distances.csv'\n",
    "indices_file = 'SalinasA_knn_spatial_embedding_pca_12_no_kernel_indices.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "distances = pd.read_csv(distances_file, header=None, low_memory=False)\n",
    "indices = pd.read_csv(indices_file, header=None, low_memory=False)\n",
    "indices = np.array(indices)\n",
    "distances = np.array(distances)\n",
    "\n",
    "# Use the existing functions to create the k-nearest neighbors graph\n",
    "A = create_knn_graph(indices, distances, k)\n",
    "\n",
    "# Apply the assertion function to the generated matrix\n",
    "try:\n",
    "    assert_graph_requirements(A, k)\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    print(\"Matrix A:\\n\", A)\n",
    "    print(\"Non-zero elements per column:\", (A > 0).sum(axis=0))\n",
    "    \n",
    "# Calculate in-degree by counting non-zero entries in each column\n",
    "in_degrees = np.sum(A > 0, axis=1)\n",
    "local_density = fast_local_clustering_coefficient(A)\n",
    "\n",
    "plot_with_matlab(eng, in_degrees, local_density, network_name=f\"SalinasA\")"
   ],
   "id": "c2a02eba694a8455",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing kNN for SalinasA:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9e541f46d7e4769968774b88fcddf68"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the adjacency matrix: 0.9938358083496778\n",
      "1. All weights are positive: PASS\n",
      "2. Each column has exactly 44 non-zero elements: PASS\n",
      "3. Matrix is column-stochastic: PASS\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:09:03.996554Z",
     "start_time": "2024-09-16T10:09:03.994994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "#\n",
    "#"
   ],
   "id": "53e0adf4a150abe8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:23:26.955485Z",
     "start_time": "2024-09-16T10:09:04.703175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pydicom\n",
    "import numpy as np\n",
    "from skimage.filters import gaussian\n",
    "from skimage.exposure import equalize_adapthist\n",
    "\n",
    "# Define the path to the DICOM folder\n",
    "folder_path = 'data/SRS00013/'  # Update this to the path of your DICOM folder\n",
    "\n",
    "# Function to convert time in HHMMSS.fff format to seconds\n",
    "def time_to_seconds(t):\n",
    "    \"\"\"Converts a time string in HHMMSS.fff format to seconds.\"\"\"\n",
    "    hours, minutes, seconds = int(t[:2]), int(t[2:4]), float(t[4:])\n",
    "    return 3600 * hours + 60 * minutes + seconds\n",
    "\n",
    "# Step 1: Reading DICOM files and extracting relevant information\n",
    "dicom_files = [f for f in os.listdir(folder_path) if f.endswith('.DCM')]\n",
    "\n",
    "# Initialize lists to store image data, acquisition times, and positions\n",
    "image_data = []\n",
    "acquisition_times = []\n",
    "image_positions = []\n",
    "acquisition_numbers = []\n",
    "instance_numbers = []\n",
    "\n",
    "# Loop through DICOM files and process each\n",
    "for file in dicom_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    ds = pydicom.dcmread(file_path)\n",
    "    \n",
    "    # Preprocess the image (Gaussian filter + adaptive histogram equalization)\n",
    "    processed_image = equalize_adapthist(gaussian(ds.pixel_array, sigma=1))\n",
    "    image_data.append(processed_image)\n",
    "    \n",
    "    # Extract acquisition time and position\n",
    "    acquisition_times.append(time_to_seconds(ds.AcquisitionTime))\n",
    "    image_positions.append(ds.ImagePositionPatient)\n",
    "    \n",
    "    acquisition_numbers.append(ds.AcquisitionNumber)\n",
    "    instance_numbers.append(ds.InstanceNumber)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "image_data = np.array(image_data)  # Shape: [T, X, Y]\n",
    "image_positions = np.array(image_positions)\n",
    "acquisition_times = np.array(acquisition_times)\n",
    "\n",
    "# Step 2: Sort the data by acquisition time and position (Z-slice position)\n",
    "# Combine image data, positions, and times into a single list of tuples\n",
    "combined = list(zip(image_data, image_positions, acquisition_times, acquisition_numbers, instance_numbers))\n",
    "\n",
    "# First, sort by acquisition time (T dimension)\n",
    "combined.sort(key=lambda x: x[2])\n",
    "\n",
    "# Then, sort by the Z-position (Y-coordinate of the image position, for slices)\n",
    "combined.sort(key=lambda x: x[1][1])\n",
    "\n",
    "# Separate the combined list back into individual lists\n",
    "image_data_sorted, image_position_sorted, acquisition_time_sorted, acquisition_numbers_sorted, instance_numbers_sorted = zip(*combined)\n",
    "\n",
    "# Convert sorted image data back to NumPy array\n",
    "image_data_sorted = np.array(image_data_sorted)  # Now sorted by time and Z-position\n",
    "\n",
    "# Step 3: Organize the data into a 4D data cube [Z, X, Y, T]\n",
    "# Let's assume you have 50 time points per Z-slice and 20 Z-slices for example:\n",
    "Z_slices = 20  # Change based on your data\n",
    "time_points = len(image_data_sorted) // Z_slices  # Number of time points per Z-slice\n",
    "\n",
    "# Initialize a 4D data cube with shape [Z, X, Y, T]\n",
    "X, Y = image_data_sorted[0].shape  # Spatial dimensions\n",
    "data_cube = np.zeros((Z_slices, X, Y, time_points))\n",
    "\n",
    "# Fill the data cube with the sorted image data, and transpose the time dimension\n",
    "for z in range(Z_slices):\n",
    "    # Transpose the sub-array so that time points are in the last dimension\n",
    "    data_cube[z, :, :, :] = np.transpose(image_data_sorted[z * time_points: (z + 1) * time_points, :, :], (1, 2, 0))\n",
    "\n",
    "# Print the shape of the data cube\n",
    "print(\"Data cube shape:\", data_cube.shape)  # Should be [Z, X, Y, T]\n",
    "\n",
    "# Assuming data_cube has the shape [Z, X, Y, T]\n",
    "Z, X, Y, T = data_cube.shape\n",
    "\n",
    "# Step 1: Reshape the data cube to flatten the (X, Y) dimensions into a single dimension\n",
    "flattened_data_cube = np.reshape(data_cube, (Z, X * Y, T))\n",
    "\n",
    "# Step 2: Verify the new shape\n",
    "print(\"New shape of data cube:\", flattened_data_cube.shape)  # Should be [Z, (X * Y), T]\n",
    "\n",
    "\n",
    "process_dataset('SRS00013', flattened_data_cube, ground_truth, method_name, param_grid, feature_para_pair, no_tuning=False, threshold=0.017)\n",
    "# Load the data from CSV files\n",
    "distances_file = 'SRS00013_knn_spatial_embedding_pca_12_no_kernel_distances.csv'\n",
    "indices_file = 'SRS00013_knn_spatial_embedding_pca_12_no_kernel_indices.csv'\n",
    "\n",
    "# Read the CSV files\n",
    "distances = pd.read_csv(distances_file, header=None, low_memory=False)\n",
    "indices = pd.read_csv(indices_file, header=None, low_memory=False)\n",
    "indices = np.array(indices)\n",
    "distances = np.array(distances)\n",
    "\n",
    "# Use the existing functions to create the k-nearest neighbors graph\n",
    "A = create_knn_graph(indices, distances, k)\n",
    "\n",
    "# Apply the assertion function to the generated matrix\n",
    "try:\n",
    "    assert_graph_requirements(A, k)\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    print(\"Matrix A:\\n\", A)\n",
    "    print(\"Non-zero elements per column:\", (A > 0).sum(axis=0))\n",
    "    \n",
    "# Calculate in-degree by counting non-zero entries in each column\n",
    "in_degrees = np.sum(A > 0, axis=1)\n",
    "local_density = fast_local_clustering_coefficient(A)\n",
    "\n",
    "plot_with_matlab(eng, in_degrees, local_density, network_name=f\"SRS00013\")"
   ],
   "id": "45329b4cf8369db3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cube shape: (20, 256, 256, 50)\n",
      "New shape of data cube: (20, 65536, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing kNN for SRS00013:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c55c299f5e64cf6adbbedae023fee0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the adjacency matrix: 0.9999664306640625\n",
      "1. All weights are positive: PASS\n",
      "2. Each column has exactly 44 non-zero elements: PASS\n",
      "3. Matrix is column-stochastic: PASS\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T10:25:00.357956Z",
     "start_time": "2024-09-16T10:25:00.335338Z"
    }
   },
   "cell_type": "code",
   "source": "eng.exit()",
   "id": "8cc8e7e248c598da",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Q5",
   "id": "9876a0ac9055313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# see .mlx",
   "id": "ac0a9dfcd4d27b68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
